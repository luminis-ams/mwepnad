{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-WePNaD task\n",
    "\n",
    "In this notebook we'll take a first look at the data for the M-WePNaD task; we'll sample ten random training queries and develop a simple preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "    libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\n",
      "    library_dirs = ['/Users/richard/anaconda/envs/mwepnad/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/Users/richard/anaconda/envs/mwepnad/include']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\n",
      "    library_dirs = ['/Users/richard/anaconda/envs/mwepnad/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/Users/richard/anaconda/envs/mwepnad/include']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\n",
      "    library_dirs = ['/Users/richard/anaconda/envs/mwepnad/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/Users/richard/anaconda/envs/mwepnad/include']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\n",
      "    library_dirs = ['/Users/richard/anaconda/envs/mwepnad/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/Users/richard/anaconda/envs/mwepnad/include']\n"
     ]
    }
   ],
   "source": [
    "# check how np is configured (do we have fast linear algebra?)\n",
    "# NB: it's not that easy to find just how to interpret this output.\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_dir = os.path.join('..', '..', 'data', 'training_data', 'MWePNaDTraining')\n",
    "exp_dir = os.path.join('..','..','exp')\n",
    "src_dir = os.path.join('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(seed=42)\n",
    "ten_queries = prng.choice(os.listdir(training_data_dir), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first peek into some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walk_first_query = os.walk(os.path.join(training_data_dir, ten_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../../data/training_data/MWePNaDTraining/paul_erhlich/001',\n",
       "  [],\n",
       "  ['001.txt', 'metadata.xml', 'SR001.htm']),\n",
       " ('../../data/training_data/MWePNaDTraining/paul_erhlich/002',\n",
       "  [],\n",
       "  ['002.txt', 'metadata.xml', 'SR002.htm'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in islice(walk_first_query, 1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_query_dir = os.path.join(training_data_dir, ten_queries[0])\n",
    "first_metadata_path = os.path.join(first_query_dir, os.listdir(first_query_dir)[0], 'metadata.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<tns:Annotation_Corpus xmlns:tns=\"http://www.example.org/metadata-corpus\" \n",
      "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n",
      "xsi:schemaLocation=\"http://www.example.org/metadata-corpus metadata-corpus.xsd\">\n",
      "<tns:url>http://es.wikipedia.org/wiki/Paul_Ehrlich</tns:url>\n",
      "<tns:language>ES</tns:language>\n",
      "<tns:downloadDate>2013-07-05</tns:downloadDate>\n",
      "<tns:annotator>Miguel Bernab√©</tns:annotator>\n",
      "</tns:Annotation_Corpus>\n"
     ]
    }
   ],
   "source": [
    "print(open(first_metadata_path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse(first_metadata_path)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element '{http://www.example.org/metadata-corpus}url' at 0x117b899f8>,\n",
       " <Element '{http://www.example.org/metadata-corpus}language' at 0x117b89a98>,\n",
       " <Element '{http://www.example.org/metadata-corpus}downloadDate' at 0x117b89ae8>,\n",
       " <Element '{http://www.example.org/metadata-corpus}annotator' at 0x117b89b38>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{http://www.example.org/metadata-corpus}Annotation_Corpus'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_e = root.find('{http://www.example.org/metadata-corpus}language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://es.wikipedia.org/wiki/Paul_Ehrlich'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.find('{http://www.example.org/metadata-corpus}url').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ES'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_e.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful ad-hoc functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NB: fix annoying spelling mistake in gold standard and dirname in training corpus\n",
    "def correct_paul_erhlich(query):\n",
    "    return 'paul ehrlich' if query == 'paul erhlich' else query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_gold_standard():\n",
    "    try:\n",
    "        f = open(os.path.join(training_data_dir, '..', 'GoldStandardTraining.txt'))\n",
    "    except IOError:\n",
    "        pass\n",
    "    else:\n",
    "        with f:\n",
    "            return frozenset([(correct_paul_erhlich(q), d, c) for q, d, c in [\n",
    "                    tuple(l.rstrip('\\n').split('\\t')) for l in f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_standard = load_gold_standard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse document dir\n",
    "\n",
    "Load data and metadata in memory\n",
    "\n",
    "Just meant as a minimal parse of the files on disk, not meant to hold many features.\n",
    "\"\"\"\n",
    "class Document:\n",
    "    XMLNS = '{http://www.example.org/metadata-corpus}'\n",
    "    def __init__(self, dir_path, file_paths):\n",
    "        self.path = dir_path\n",
    "        self.file_paths = file_paths\n",
    "        self._parse_metadata()\n",
    "        self._get_text()\n",
    "        \n",
    "    def _parse_metadata(self):\n",
    "        tree = ET.parse(os.path.join(self.path, 'metadata.xml'))\n",
    "        root = tree.getroot()\n",
    "        self.url_ = root.find('{}url'.format(Document.XMLNS)).text\n",
    "        self.languages = frozenset(root.find('{}language'.format(Document.XMLNS))\n",
    "                                   .text.strip().split(','))\n",
    "         \n",
    "    def _get_text(self):\n",
    "        ids = [re.sub(r'\\.txt$', '', p) for p in self.file_paths if p.endswith('.txt')]\n",
    "        assert len(ids) == 1\n",
    "        self.id = ids[0]\n",
    "        with open(os.path.join(self.path,'{}.txt'.format(self.id))) as f:\n",
    "            self.text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse query dir\n",
    "\n",
    "Load data and metadata in memory\n",
    "\"\"\"\n",
    "class Query:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.query = correct_paul_erhlich(os.path.basename(path).replace('_', ' '))\n",
    "        try:\n",
    "            self._load()\n",
    "        except Exception as e:\n",
    "            log('Exception: {}'.format(e))\n",
    "            raise ValueError('Could not parse everything in query dir')\n",
    "    \n",
    "    def _load(self):\n",
    "        self.docs = []\n",
    "        walker = os.walk(self.path)\n",
    "        for path, _, files in walker:\n",
    "            if 'metadata.xml' in files:\n",
    "                self.docs.append(Document(path, files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A closer look at our first query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_query = Query(first_query_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/training_data/MWePNaDTraining/paul_erhlich'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_query.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paul ehrlich'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_query.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_docs_first_query = pd.Series([doc.languages for doc in first_query.docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([frozenset({'ES'}), frozenset({'EN'}), frozenset({'ES', 'EN'}),\n",
       "       frozenset({'DE'}), frozenset({'DE', 'EN'})], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_docs_first_query.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paul r. ehrlich (paulrehrlich) en twitter twitter consulta de b√∫squeda buscar cuenta verificada @ idioma: espa√±ol bahasa indonesia bahasa melayu dansk deutsch english englishuk euskara filipino galego italiano lolcatz magyar nederlands norsk polski p\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\s+', ' ', first_query.docs[11].text.lower())[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_first_query_urls = pd.Series([doc.url_ for doc in first_query.docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18    http://www.biography.com/people/paul-ehrlich-9...\n",
       "45         http://www.allergyasthmanyc.com/bio_paul.php\n",
       "47    http://www.patheos.com/blogs/godandthemachine/...\n",
       "89         http://en.wikiquote.org/wiki/Paul_R._Ehrlich\n",
       "4     http://www.stanford.edu/group/CCB/cgi-bin/ccb/...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_first_query_urls.sample(5, random_state=prng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for classification into 'NR' and 'relevant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute a feature\n",
    "\n",
    "Accepts a Document and a Query object. \n",
    "Normally you would use this function with a Document that is in query.docs.\n",
    "\n",
    "Returns a numeric value\n",
    "\"\"\"\n",
    "def compute_n_exact_name_matches(query, doc):\n",
    "    first_name, last_name = query.query.split(' ')\n",
    "    re_ = re.compile(r'' + re.escape(first_name) + r'\\s+' + re.escape(last_name))\n",
    "    return len(re.findall(re_, doc.text.lower()))\n",
    "    # TODO: use URL, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute a feature\n",
    "\n",
    "Accepts a Document and a Query object. \n",
    "Normally you would use this function with a Document that is in query.docs.\n",
    "\n",
    "Returns a numeric value\n",
    "\"\"\"\n",
    "def compute_n_name_matches_with_optional_word_or_initial_in_between(query, doc):\n",
    "    first_name, last_name = query.query.split(' ')\n",
    "    re_ = re.compile(r'' + re.escape(first_name) + r'\\s+' + r'[\\w]*\\.?\\s*' + re.escape(last_name))\n",
    "    return len(re.findall(re_, doc.text.lower()))\n",
    "    # TODO: use URL, too\n",
    "    # TODO: compute other features, e.g., 1 / (1 + number_of_chars_between_first_and_last_name)\n",
    "    # TODO: possibly preprocess and tokenise text / URL prior to computing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_n_exact_name_matches(first_query, first_query.docs[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_n_name_matches_with_optional_word_or_initial_in_between(first_query, first_query.docs[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get an idea for how well these two features correlate with NR in the ground truth for our ten randomly sampled development queries. Or rather, first, just for our 'first_query'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_feature = pd.Series({\n",
    "        doc.id : compute_n_name_matches_with_optional_word_or_initial_in_between(\n",
    "                first_query, doc) for doc in first_query.docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_gold_standard = pd.Series({a[1] : 1 if a[2] == 'NR' else 0 for a in gold_standard if a[0] == first_query.query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([s_gold_standard, s_feature], axis=1, join_axes=[s_gold_standard.index], keys=['NR', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NR</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.293478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x\n",
       "NR           \n",
       "0    9.293478\n",
       "1   12.428571"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('NR').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, above, the number of name mention matches is higher for pages annotated as NR. What reasons would annotators use to make this decision? Let's look into some of these pages, where we do have name matches, but the pages are annotated as 'NR' (not relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>009</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>022</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>024</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>027</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>029</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>033</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>066</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NR   x\n",
       "009   1   0\n",
       "022   1   5\n",
       "024   1  43\n",
       "027   1   3\n",
       "029   1  28\n",
       "033   1   3\n",
       "066   1   5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['NR'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfiles: Paul Ehrlich | LinkedIn\n",
      "\n",
      "\tInicio\n",
      "\t¬øQu√© es LinkedIn?\n",
      "\t√önete hoy\n",
      "\tInicia sesi√≥n\n",
      "\n",
      "B√∫squeda por nombre\n",
      "\t\n",
      "Nombre\n",
      "\n",
      "\t\n",
      "Apellidos\n",
      "\n",
      "Paul Ehrlich\n",
      "\n",
      "\t\n",
      "25 de 42 perfiles\n",
      "| Ver todos los perfiles en LinkedIn ¬ª\n",
      "\n",
      "\t\n",
      "\t\n",
      "Ver el perfil completo\n",
      "\n",
      "\t\n",
      "\n",
      "Paul\n",
      "Ehrlich\n",
      "\n",
      "\tCargo\n",
      "\tChief Medical Officer, Cerner Corporation\n",
      "\tInformaci√≥n demogr√°fica\n",
      "\t\n",
      "\n",
      "Kansas City y alrededores, Missouri, Estados Unidos\n",
      "\n",
      " | \n",
      "\n",
      "Atenci√≥n sanitaria y hospitalaria\n",
      "\n",
      "\tActual:\n",
      "\t\n",
      "\n",
      "Chief Medical Officer at Cerner Corporation\n",
      "\n",
      "\tAnterior:\n",
      "\t\n",
      "VP and C\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\n\\n+', '\\n\\n', [doc for doc in first_query.docs if doc.id == '029'][0].text)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above page seems to be a LinkedIn listing of public profiles that came up for the search 'Paul Ehrlich'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Ehrlich MedChem Euro-PhD\n",
      "\n",
      "Paul\n",
      "Ehrlich European Medicinal Chemistry Ph.D. Network\n",
      "\n",
      "The Doctorate Course in Pharmaceutical Chemistry¬†at the University of Vienna is part of a European network, recently formed, which has the aim\n",
      "¬†of ¬†fostering the education and research training of\n",
      "post-graduate students in Medicinal Chemistry towards PhD degree. In\n",
      "particular the aim of the Paul Ehrlich MedChem Euro-PhD Network is to\n",
      "provide an in-depth research training and mobility of PhD students in\n",
      "the ar\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\n\\n+', '\\n\\n', [doc for doc in first_query.docs if doc.id == '024'][0].text)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above page seems to refer mainly to an event / network that is named after one Paul Ehrlich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some ideas for this task\n",
    "\n",
    "As a first step, it would seem a nice experiment to see if we can correctly classify pages as 'NR' (binary classification). Because of the way this task will be evaluated, it is a good idea to put all of these pages together in a single cluster. We could engineer some features (see ideas in the TODO's above) and then train a simple classifier.\n",
    "\n",
    "As a second step, we could try to find some generic way to tokenise text, regardless of language. So, no stemming or anything like that yet, just tokenisation. Then, an often used representation of documents could be a TF-IDF vector.\n",
    "\n",
    "As a third step, we could calculate cosine distances between documents.\n",
    "\n",
    "As a fourth step, we could use hierarchical agglomerative clustering, it has performed well in previous editions of WePS campaigns.\n",
    "\n",
    "If we want, we can use the simple idea from Berendsen et al, ECIR 2012, where social media profiles were not included in the clustering. Instead, adding them as singleton clusters after clustering the rest of the pages boosted the score considerably.\n",
    "\n",
    "If we want to improve further, we could add custom tokenisation for each language.\n",
    "\n",
    "We can also investigate how common it is that an individual will be referred to from pages with different languages. If it happens a lot, another way to improve scores would be to use some kind of translation machinery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
